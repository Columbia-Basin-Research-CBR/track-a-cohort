---
title: "utils_compare_BOR_SacPAS_predicted_weekly_loss"
---

This document is the R script for comparing the predicted weekly loss from the BOR provided model code and the Loss and Salvage Predictor Tool currently hosted on SacPAS. The SacPAS tool R code was provided by Nick Beer: see `data-raw/calibrate.R` for script. The BOR model code was provided by the BOR: see `data-raw/utils_fct_predict_tillotson_model.R` and `WR_Model_Setup.R` and `Steelhead_Model_Setup.R` for script.

Outstanding questions/thoughts:

1.  What role does brt.functions play in the BOR model code? As far as I can tell, it is not used in the model code.

2.  Does the ntrees = 1000 for the training dataset in NB shared code change the outcome?

    -   Changes decimal points if both test and train are set to 1000.
    -   Tillotson et. al (2022) states all default values are used except for ntrees which is set to 300 for the quantile regression forest model. "The models were trained using the ‘quantregForest’ (Meinshausen 2017), in R (R Core Team 2019). A range of values for key algorithm parameters were tested, including the number of variables sampled at each tree split (mtry), the minimum number of terminal nodes (nodesize) and the number of trees in the forest (ntree), but the defaults were ultimately deemed sufficient aside from tree number, which was set to 300 for the sake of computational efficiency (Oshiro et al. 2012)."
    -   Keep at 500 or switch to 300 to match paper?

3.  NB shared code allows full train data set (1999:2020) or new calibration (2009:2020) -- BOR code uses 1999:2021? Which to use and rationale supporting? Should we offer both options in TAC code?

    -   Per Tillotson et al (2022): "We also limited predictor variables to those with data available for the entire salvage time-series (WY 1999–2020), and gave preference to variables with readily available forecasts. For model training, we aggregated all variables from daily observations to weekly means, except for precipitation and prior week salvage, which were summed, and Delta Cross Channel (DCC) gate status, which was listed as “opened” or “closed” based on a simple majority of daily statuses." ... "The predictive model was initially developed as a single-step QRF model (hereafter the simple model), trained with data described above for water Years 1999-2020."

4.  Further, all train datasets differ in observations. NB uses `AllYears.Intake.csv` while BOR uses `ITMData.rda` which includes df.ss and df.week from WY1999 to 2022. NB shared code uses kNNimputation() to fill missing covariate data (uses DMwR package, since removed from CRAN, tried using VIM::kNN() instead). Whereas, BOR shared code does not fill in missing doy covariates. However, this method is not mentioned in either Tillotson et al (2022) or BOR shared code. Additionally, observations for covariates differ from NB to BOR shared code, with the former rounding data values and the later leaving as is. Either way they seem to differ with/without rounding. Unsure why df.ss and df.week differ in observations but were both included in `ITMData.rda`, values are pretty close for covariates.

    -if possible, I think going forward we pull historical training data sets form source (via SacPAS) to ensure consistency in training data sets.

5.  Also, shared BOR code sets precipitation to 1, whereas NB shared code aggregates by mean `precip <- aggregate(precip ~ year.week, data = df.covars.filled, mean)[,2]`. This all differs from stated Tillotson etl (2022) methods:

    -   "...For model training, we aggregated all variables from daily observations to weekly means, except for precipitation and prior week salvage, which were summed, and Delta Cross Channel (DCC) gate status, which was listed as “opened” or “closed” based on a simple majority of daily statuses."

    -   Tillotson et al (2022) uses \[DAYflow\](https://data.ca.gov/dataset/dayflow); recommendes looking into method to incorporate accumlated precipitation: `I think would also be worth your time to see whether the SacPAS implementation of the model is appropriately adding the apportioned precipitation from prior days (e.g. If it rained five inches on day one, five inches on day two and zero inches thereafter, daily precip would be 1/0.00017448  on day one, (1+1)/0.00017448 on days two through five and 1/0.00017448 on day six). That's not something I had to deal with when developing the model as I was just relying on the historic DAYFLOW data which already accounted for all steps in calculating precipitation. `

6.  Since the BOR and NB code is using binary.form ="none" and quantile.form = "qrf", is appears no random forest, `randomForest`, is used, just a quantile regression forest `quantrefForest` which differs from Tillotson et al (2022):

    -   "Feedback from potential model users included some concern regarding the reliance of the models on the prior week’s loss since it may lead to poor prediction early in the salvage season when few fish are observed. In an attempt to address this concern, we developed an alternative, two-step model formulation (hereafter referred to as the hurdle model). The hurdle model first trains a random forest classifier using the ‘randomForest’ package (Law and Wiener 2002). The response and predictor variables are the same as for the simple model, except that the responses are converted from continuous to binary (i.e., loss/no loss) and the prior week’s loss is excluded. A QRF model with the same formulation as the simple model is then fit to a censored data set containing only weeks with non-zero observed loss. The prediction from the classification model is then multiplied by the prediction in the simple model. Thus, the first step (RF classifier) predicts whether or not any loss will occur in the coming week based only on environmental and operational conditions, and when loss is predicted to occur, the second step (QRF regression) estimates the magnitude of loss using the complete set of predictors, (i.e., including the prior week’s loss). Per each response variable, both the simple and hurdle models were fit. were fit. The output of each model is the same: a predicted distribution of the response variable, which can be used to explore the range of potential outcomes, given a set of environmental and operational conditions "
    -   ...but later states..."Examination of prediction errors during the first week of each salvage season in which greater than 5% of the annual loss occurred revealed poor predictive ability in these circumstances across all model formulations, with large under-predictions the norm (Figure 6A). In contrast, across all weeks in which greater than 5% of annual loss occurred, the 75th predicted quantiles for both species and all model formulations produced more balanced prediction errors (Figure 6B). In addition to indicating that the hurdle model formulation may be unnecessary, these results also suggest that use of a more precautionary management benchmark, such as the 90th quantile (Figure 6C and 6D), may be warranted early in the year when regular weekly loss is not yet occurring."
    -   Binary hurdle model unnecessary deemed unnecessary then?

7.  No cross validation is done on either NB or BOR shared code. In paper, it states the strongest variables in predicting loss was the prior week's loss and the week of the water year, which was meant to be handled by the additional hurdle model fitting, but also why the more conservative LOO CV method was selected.

    -   "...the 10-fold approach produced consistently more optimistic results (i.e., suggested better predictive performance). This probably results from the fact that there is an unaccounted-for effect of year in determining the risk of entrainment and removing an entire year of data ensures that the model is trained completely naively to this annual effect. For the sake of clarity, we have chosen to report only the results from the more conservative LOO cross-validation. "

8.  In `calibrate.R` script shared by NB, there was some question regarding which columns were being called such that if spring observed loss was being called instead of steelhead. NB looked into and it appears the observed data is reflecting properly. Note for future self, column steelhead = take, whereas stlhd_loss = observed loss, winter and spring = observed loss and winter_cl and spring_cl = clipped loss. The code might still be dropping percip and calling df.covar for spring and using imputation to fill based on missing spring dates, but this is not clear in the code.

**Meeting notes (JG & COB), 10/3/2024:**

main idea: COB to make a write up with comparison findings and recommendations/questions for NB and confirmation from JG. Share via github. Below outlines some specifics:

-   Training dataset:

    -   Confirm with BOR which training dataset is preferred, 2009:CY or all years 1999:CY. When comparing within loss and salvage predictor tool, there was a difference in loss reported with all years predicting higher loss than 2009:2020 years (77.44 compared to 38.97 respectively, Example was June 8, 2023).

        -   Determine rationale behind selection– Jenn to ask BOR.

    -   confirm NB is rerunning training data set with up to date years, or just adding new data to existing model as testing data (newdata)

    -   confirm with NB what code is doing with running data through with each year of data.

    -   decide it TAC will match NB code to fill in missing covariate data with knn imputation.

    -   Caitlin & NB (if he isn't already) to pull data from DART instead of Tillotson/BOR supplied code for training dataset.

-   Model:

    -   Set ntrees to default setting. Caitlin to confirm if that is 300 or 500.

    -   Precipitation, stick with sum of (5-day) precipitation (see Tillotson for link. Not available on SacPAS but NB must be getting the data. Request on MC to import, but possible it's similar to mal_temp and SI is already importing and just needs to update query tool.

        -   JG questions why 5 -day precipitation? Look into why it isn’t a 7 day. Paper mentions its cfs but link shows inches. Missing something? Compare precip results to shared static files for comparision then switch.

    -   Model stick with quantreg but compare with adding random forest - just for comparison

    -   Add CV for 10fold and LOO and no CV – for comparison

This document is the R script for comparing the predicted weekly loss from the BOR provided model code and the Loss and Salvage Predictor Tool currently hosted on SacPAS. The SacPAS tool R code was provided by Nick Beer: see `data-raw/calibrate.R` for script.  The BOR model code was provided by the BOR: see `data-raw/utils_fct_predict_tillotson_model.R` and `WR_Model_Setup.R` and `Steelhead_Model_Setup.R` for script. 



```{r load_libraries, message=FALSE, warning=FALSE}
library(tidyverse)
library(here)
library(randomForest)
library(quantregForest)
library(plotly)
#load BOR shared resources
load(here("R/ITMData.rda")) #return 1) df.ss and 2) df.week from 1999 to 2021

# #shared model output from Nick Beer -- used to calibrate
load(here::here("data-raw", "FullFits.Original.R")) #seems to be model output for each of the three options: 1) Steelhead, 2) Winter-run Chinook, 3) Spring-run Chinook
```

```{r compare_train_datasets}

#seems to have all data foe every day of the week -- this likely was populated usng the kNNimputation() to fill out?
train_nb<-read.csv(here::here("data-raw/AllYears.Intake.csv")) %>%  
  mutate(date = lubridate::ymd(date))

# Identify dates in train.nb that are not in df.ss
missing_dates_train_nb <- anti_join(train_nb, df.ss, by = "date") %>% select(date) #8,234 obs in train.nb not in df.ss

# Identify dates in df.week that are not in df.ss
missing_dates_df_week <- anti_join(df.week, df.ss, by = "OMR") %>% select(OMR) #286 obs in df.week not in df.ss

#identify dates in train.nb that are not in df.week
missing_dates_train_week <- anti_join(train_nb, df.week, by = "date") %>% select(date) #7,948 obs in train.nb not in df.week
  
```

```{r compare_precipitation}
#compare precipitation values between NB and BOR shared code

#NB shared code
train_nb<-read.csv(here::here("data-raw/AllYears.Intake.csv")) #9136 obs, 1999-2023 
  train_nb$year_week <- train_nb$water_year + train_nb$wk_dec
  

   sum_train_nb<-train_nb %>% 
    select(date, water_year, wy_week, year_week, wk_dec, precip) %>% 
     filter()
     group_by( year_week) %>% 
      summarise(mean_precip = mean(precip, na.rm = TRUE), 
                sum_precip = sum(precip, na.rm = TRUE)) #summed precip value matches value in df.ss 

#BOR shared training data
train_bor <- df.ss #1999-2021, 901 obs

sum_train_nb 
sum_bor

#convert back to inches
train_inches<-train_nb %>% 
  filter(water_year == 2023) %>% 
  select(date, water_year, wy_week,year_week, month, julian, precip) %>% 
  mutate(precip_inches = precip * 0.00017448)

#SI notes: daily value divided by .00017448 then summed for 5 days. the constant value of 0.00017448 would represent the area and rate of the river to get the cfs value, or the measure of the volume of water passing a specific point in the river per second. Q: where does the 5-day sum start from? why then grouped by water year week (7-day) and not 5-day? and Where is this conversion factor coming from?


#conversion formula 
# cfs =  (precipitation (inches) * (1/12) * area)/86,400
#suggested conversion method by Tillotson email: 
# Inches at Stockton times Delta area as given in DAYFLOW documentation (acre inches/ day)
# Divide by 12 (acre feet/ day)
# Multiply by 43,556  (cubic feet/ day)
# Divide by 86,400, the number of seconds in a day (CFS)
# Divide by 5, DAYFLOW methods assume precipitation enters the Delta evenly over the day of rainfall and the next four days.
 

```

```{r import_new_loss_data}

#shared model output from Nick Beer -- used to calibrate
load(here("data-raw", "Full.Fits.Original.R")) #seems to be model output for each of the three options: 1) Steelhead, 2) Winter-run Chinook, 3) Spring-run Chinook 
```

```{r test_train_data}
# assign current water year
source(here("R/utils_fct_assign_current_water_year.R"))
current_year <- assign_current_water_year()

startOfWY <- function(date) {
  if (month(date) >= 10) {
    ymd(paste(year(date), "10-01", sep = "-"))
  } else {
    ymd(paste(year(date) - 1, "10-01", sep = "-"))
  }
}

calculateWYWeek <- function(date) {
  waterYearStart <- startOfWY(date)
  daysSinceStart <- as.integer(difftime(date, waterYearStart, units = "days"))
  weekNumber <- ceiling(daysSinceStart / 7)
  return(weekNumber)
}

weekStartDate <- function(weekNumber, currentDate) {
  waterYearStart <- startOfWY(currentDate)
  startDateOfWeek <- waterYearStart + days((weekNumber - 1) * 7)
  return(startDateOfWeek)
}

winter_run_url <- paste0("https://www.cbr.washington.edu/sacramento/data/php/rpt/juv_loss_detail.php?sc=1&outputFormat=csv&year=", current_year, "&species=1%3Af&dnaOnly=no&age=no")

df_fish_raw <- read_csv(winter_run_url) %>%
    janitor::clean_names()
  
df_fish_winter  <- df_fish_raw %>% 
  filter(lad_race == "Winter") %>% 
    mutate(
      sample_time = ymd_hms(sample_time),
      date = date(sample_time)
    ) %>%
    group_by(date) %>%
    summarise(total_daily_loss = sum(loss)) %>%
    ungroup() %>%
    mutate(week = sapply(date, calculateWYWeek)) %>%
    group_by(week) %>%
    summarise(total_weekly_loss = sum(total_daily_loss)) %>% 
    mutate(calendar_date = weekStartDate(week, Sys.Date()))  # Add calendar date column



```


```{r import_new_river_data}

#load function to import river data
source(here::here("data-raw/utils_fct_import_river_data.R"))

# Set variables of interest for river data import function
  # To get current WY years of data:
  today <- Sys.Date()
  # Determine the current and previous water years based on today's date
  if (format(today, "%m") >= "10") {
    currentWY <- as.numeric(format(today, "%Y")) + 1
  } else {
    currentWY <- as.numeric(format(today, "%Y"))
  }
  previousWY <- currentWY - 1
  # return years of interest
  years <- previousWY:currentWY

  # select sites of interest: Flow, OMR, Export, Temperature
  code_list <- c("FPT", "VNS", "MAL", "TRP", "HRO")
  # set years of interest
  years <- c(years)
  # selected metrics of interest
  metrics <- c("Flow", "WaterTemperature", "PumpingDischarge")

  # Run the function with the code list, years, and metrics
  df_river_raw <- fct_import_SacPAS_river_conditions_query(sites = code_list, years = years, metrics = metrics)
  # for some reason function returns OMR data multiple times when run in a list above. For now running seperately
  df_OMR_raw <- fct_import_SacPAS_river_conditions_query(sites = "Combined", years = years, metrics = "OMRDailyUSGS")

  df_river <- df_river_raw %>%
    bind_rows(df_OMR_raw) %>%
    mutate(WY = year(YMD) + (month(YMD) >= 10)) %>%
    filter(WY == current_year) %>%
    mutate(week = sapply(YMD, calculateWYWeek)) %>%
    pivot_wider(names_from = c(location, parameter), values_from = value) %>%
    select(YMD, WY, week, FPT_flow, VNS_flow, Combined_OMRDailyUSGS, TRP_pumping, HRO_pumping, MAL_wtemp) %>%
    mutate(combined_export = TRP_pumping + HRO_pumping) %>%
    group_by(week) %>%
    summarise(
      weekly_avg_fpt_flow = mean(FPT_flow, na.rm = TRUE),
      weekly_avg_vns_flow = mean(VNS_flow, na.rm = TRUE),
      weekly_avg_omr = mean(Combined_OMRDailyUSGS, na.rm = TRUE),
      weekly_avg_export = mean(combined_export, na.rm = TRUE),
      weekly_avg_mal_wtemp = mean(MAL_wtemp, na.rm = TRUE)
    )
  
  
```


## Shared Model Output

```{r track_a_cohort_output}
source(here("R/winter_run_chinook_plot_tillotson_predicted_loss.R"))

source(here("R/steelhead_plot_tillotson_predicted_loss.R"))
```

-------------------past code

```{r}

```


```{r model_sacpas}
#required to run model
# source(here("R/brt.functions.R")) #seems to use gbm to determine howpredicted values are returned - customized function?
load(here("R/ITMData.rda")) #seems to be the data used to train the model, return 1) df.ss and 2) df.week from 1999 to 2021



  # Compute full Training set fits
  qrf.steelhead  <-quantregForest(x=df.week[,predictors.steelhead],y=df.week$steelhead,
                                 ntree = 1000,
                                 importance = T,
                                 proximity = T,
                                 keep.forest = T,
                                 keep.inbag = T,
                                 do.trace = 100,
                                 rsq = T)
  qrf.winter <-quantregForest(x=df.week[,predictors.winter],y=df.week$winter,
                              ntree = 1000,
                              importance = T,
                              proximity = T,
                              keep.forest = T,
                              keep.inbag = T,
                              do.trace = 100,
                              rsq = T)
```

## Shared Model Output
```{r}
#required to run model
source(here("R/brt.functions.R")) #seems to use gbm to determine howpredicted values are returned - customized function?
load(here("R/ITMData.rda")) #seems to be the data used to train the model, return 1) df.ss and 2) df.week from 1999 to 2021

df.ss <- df.ss%>%mutate(dcc= if_else(dcc==2,"open","closed"))

#removed all unused selections from hurdle.model function in spp_setup.R
hurdle.model <- function(data, response, 
                         quantile.predictors, 
                         predict.data = NA,
                         which.quant = .5,
                         n.trees = 500,
                         quantile.nodesize = 10,
                         log.quant = F,
                         ...) {
  resp.loc <- which(colnames(data) == response)
  
  # Use the entire dataset for quantile regression (if(pres.only == F))
  data.pres <- data.frame(data)
  
  # Train the quantile regression forest model (  if(quantile.form=="qrf"&binary.form!="none"))
  quantile.model <- quantregForest(x = data.pres[, quantile.predictors], y = data.pres[, resp.loc],
                                   ntree = n.trees,
                                   importance = TRUE, 
                                   proximity = TRUE,
                                   keep.forest = TRUE,
                                   keep.inbag = TRUE,
                                   rsq = TRUE,
                                   nodesize = quantile.nodesize)
  
  # Make predictions
  #if prediction data = NA, then use the data provided (training data)
  if (is.na(predict.data)) {
    prediction.a <- predict(quantile.model, newdata = data, what = which.quant)
    Pred_Mean <- predict(quantile.model, newdata = data, what = mean)
    #else, if prediction data is provided, use new data (test data)
  } else {
    prediction.a <- predict(quantile.model, newdata = predict.data, what = which.quant)
    Pred_Mean <- predict(quantile.model, newdata = predict.data, what = mean)
  }
  
  prediction <- data.frame(cbind(prediction.a, Pred_Mean))
  
  return(list(quantile.model, prediction))
}

```


```{r model_winterun}

#not working for some reaason - instead just run original code and and pull the predicted table to extract the new data used to predict per week and run through with NB.code 
df_combined_winter<-winter_run_tillotson_output[[2]] #


#model run
names(df.ss)
#columns call for winter-run chinook  input to quantile.predictors
names(df.ss[c(2:4,17:21,27)])


#change dcc to open or closed instead of numerical values
df.ss <- df.ss%>%mutate(dcc= if_else(dcc==2,"open","closed"))
df.week <- df.week%>%mutate(dcc= if_else(dcc==2,"open","closed")) #attempting to try with df.week but getting error--why? start here


# run model

WR_Simple_Combined <- hurdle.model(data = df.ss,
                                      response = "winter",
                                      binary.predictors = NA,
                                      quantile.predictors = c(
                                        "wy_week", 
                                        "mal_temp",
                                        "precip", 
                                        "OMR", 
                                        "sac", 
                                        "sjr",
                                        "dcc",
                                        "daily_exports",
                                        "winter.pw"
                                        ),
                                      binary.form = "none",
                                      quantile.form = "qrf",
                                      which.quant = c(0.01,0.05,.1,.25,.5,.75,.9,.95,.99),
                                      n.trees=500,
                                      quantile.nodesize = 5,
                                      pres.only=F
)


#combine fish and river for winter
df_combined_winter <- df_fish_winter %>%
    left_join(df_river, by = "week")

# Initialize a list to store predictions
predictions_list <- list()

# Loop through each row in df_combined_winter to make predictions
for (i in 1:nrow(df_combined_winter)) {
  NewData <- data.frame(
    wy_week = df_combined_winter$week[i] + 1,
    mal_temp = df_combined_winter$weekly_avg_mal_wtemp[i],
    precip = 1,
    OMR = df_combined_winter$weekly_avg_omr[i],
    sac = df_combined_winter$weekly_avg_fpt_flow[i],
    sjr = df_combined_winter$weekly_avg_vns_flow[i],
    dcc = "closed",
    daily_exports = df_combined_winter$weekly_avg_export[i],
    species.pw = df_combined_winter$total_weekly_loss[i]
  )

  # Dynamically rename the species.pw column
  names(NewData)[names(NewData) == "species.pw"] <- "winter.pw"
  
  # Access the quantile regression forest model from the list

  quantile_model <- WR_Simple_Combined[[1]] #changed to 1 versus 2 -- why did call change in listed value for WR_Simple_Combined?

  quantile_model <- WR_Simple_Combined[[2]]

  
  # Make predictions using the quantile regression forest model
  pred <- predict(quantile_model, newdata = NewData, what = c(0.05, 0.5, 0.95))
  predictions <- data.frame(
    lowerCI = pred[, 1], median = pred[, 2], upperCI = pred[, 3],
    week = df_combined_winter$week[i],
    OMR = df_combined_winter$weekly_avg_omr[i],
    Export = df_combined_winter$weekly_avg_export[i],
    ObservedLoss = df_combined_winter$total_weekly_loss[i],
    weekly_avg_mal_wtemp = df_combined_winter$weekly_avg_mal_wtemp[i],
    weekly_avg_fpt_flow = df_combined_winter$weekly_avg_fpt_flow[i],
    weekly_avg_vns_flow = df_combined_winter$weekly_avg_vns_flow[i],
    calendar_date = df_combined_winter$calendar_date[i]  # Include calendar date in predictions
  )
  
  # Store the predictions in the list
  predictions_list[[i]] <- predictions
}

# Combine all predictions into a single data frame
final_predictions <- bind_rows(predictions_list)

# Print the final predictions
print(final_predictions)

```







```{r model_steelhead}
#column calls for steelhead input to quantile.predictors
names(df.ss[c(2:4,17:21,29)])


Stlhd_Simple_Combined <- hurdle.model(data = df.ss,
                                      response = "stlhd_loss",
                                      binary.predictors = NA,
                                      quantile.predictors = c(
                                        "wy_week", 
                                        "mal_temp",
                                        "precip", 
                                        "OMR", 
                                        "sac", 
                                        "sjr",
                                        "dcc",
                                        "daily_exports",
                                        "stlhd_loss.pw"
                                        ),
                                      binary.form = "none",
                                      quantile.form = "qrf",
                                      which.quant = c(0.01,0.05,.1,.25,.5,.75,.9,.95,.99),
                                      n.trees=500,
                                      quantile.nodesize = 5,
                                      pres.only=F
)


```
