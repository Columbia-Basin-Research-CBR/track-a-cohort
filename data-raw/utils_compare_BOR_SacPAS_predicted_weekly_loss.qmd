---
title: "utils_compare_BOR_SacPAS_predicted_weekly_loss"
---

This document is the R script for comparing the predicted weekly loss from the BOR provided model code and the Loss and Salvage Predictor Tool currently hosted on SacPAS. The SacPAS tool R code was provided by Nick Beer: see `data-raw/calibrate.R` for script. The BOR model code was provided by the BOR: see `data-raw/utils_fct_predict_tillotson_model.R` and `WR_Model_Setup.R` and `Steelhead_Model_Setup.R` for script.

Outstanding questions/thoughts:

1.  What role does brt.functions play in the BOR model code? As far as I can tell, it is not used in the model code.

2.  Does the ntrees = 1000 for the training dataset in NB shared code change the outcome?

    -   Changes decimal points if both test and train are set to 1000.
    -   Tillotson et. al (2022) states all default values are used except for ntrees which is set to 300 for the quantile regression forest model. "The models were trained using the ‘quantregForest’ (Meinshausen 2017), in R (R Core Team 2019). A range of values for key algorithm parameters were tested, including the number of variables sampled at each tree split (mtry), the minimum number of terminal nodes (nodesize) and the number of trees in the forest (ntree), but the defaults were ultimately deemed sufficient aside from tree number, which was set to 300 for the sake of computational efficiency (Oshiro et al. 2012)."
    -   Keep at 500 or switch to 300 to match paper?

3.  NB shared code allows full train data set (1999:2020) or new calibration (2009:2020) -- BOR code uses 1999:2021? Which to use and rationale supporting? Should we offer both options in TAC code?

    -   Per Tillotson et al (2022): "We also limited predictor variables to those with data available for the entire salvage time-series (WY 1999–2020), and gave preference to variables with readily available forecasts. For model training, we aggregated all variables from daily observations to weekly means, except for precipitation and prior week salvage, which were summed, and Delta Cross Channel (DCC) gate status, which was listed as “opened” or “closed” based on a simple majority of daily statuses." ... "The predictive model was initially developed as a single-step QRF model (hereafter the simple model), trained with data described above for water Years 1999-2020."

4.  Further, all train datasets differ in observations. NB uses `AllYears.Intake.csv` while BOR uses `ITMData.rda` which includes df.ss and df.week from WY1999 to 2022. NB shared code uses kNNimputation() to fill missing covariate data (uses DMwR package, since removed from CRAN, tried using VIM::kNN() instead). Whereas, BOR shared code does not fill in missing doy covariates. However, this method is not mentioned in either Tillotson et al (2022) or BOR shared code. Additionally, observations for covariates differ from NB to BOR shared code, with the former rounding data values and the later leaving as is. Either way they seem to differ with/without rounding. Unsure why df.ss and df.week differ in observations but were both included in `ITMData.rda`, values are pretty close for covariates. 

    -if possible, I think going forward we pull historical training data sets form source (via SacPAS) to ensure consistency in training data sets. 

5. Also, shared BOR code sets precipitation to 1, whereas NB shared code aggregates by mean `precip <- aggregate(precip ~ year.week, data = df.covars.filled, mean)[,2]`. This all differs from stated Tillotson etl (2022) methods: 

    - "...For model training, we aggregated all variables from daily observations to weekly means, except for precipitation and prior week salvage, which were summed, and Delta Cross Channel (DCC) gate status, which was listed as “opened” or “closed” based on a simple majority of daily statuses."

6.  Since the BOR and NB code is using binary.form ="none" and quantile.form = "qrf", is appears no random forest, `randomForest`, is used, just a quantile regression forest `quantrefForest` which differs from Tillotson et al (2022):

    -   "Feedback from potential model users included some concern regarding the reliance of the models on the prior week’s loss since it may lead to poor prediction early in the salvage season when few fish are observed. In an attempt to address this concern, we developed an alternative, two-step model formulation (hereafter referred to as the hurdle model). The hurdle model first trains a random forest classifier using the ‘randomForest’ package (Law and Wiener 2002). The response and predictor variables are the same as for the simple model, except that the responses are converted from continuous to binary (i.e., loss/no loss) and the prior week’s loss is excluded. A QRF model with the same formulation as the simple model is then fit to a censored data set containing only weeks with non-zero observed loss. The prediction from the classification model is then multiplied by the prediction in the simple model. Thus, the first step (RF classifier) predicts whether or not any loss will occur in the coming week based only on environmental and operational conditions, and when loss is predicted to occur, the second step (QRF regression) estimates the magnitude of loss using the complete set of predictors, (i.e., including the prior week’s loss). Per each response variable, both the simple and hurdle models were fit. were fit. The output of each model is the same: a predicted distribution of the response variable, which can be used to explore the range of potential outcomes, given a set of environmental and operational conditions "
    -   ...but later states..."Examination of prediction errors during the first week of each salvage season in which greater than 5% of the annual loss occurred revealed poor predictive ability in these circumstances across all model formulations, with large under-predictions the norm (Figure 6A). In contrast, across all weeks in which greater than 5% of annual loss occurred, the 75th predicted quantiles for both species and all model formulations produced more balanced prediction errors (Figure 6B). In addition to indicating that the hurdle model formulation may be unnecessary, these results also suggest that use of a more precautionary management benchmark, such as the 90th quantile (Figure 6C and 6D), may be warranted early in the year when regular weekly loss is not yet occurring."
    -   Binary hurdle model unnecessary deemed unnecessary then?

7.   No cross validation is done on either NB or BOR shared code. In paper, it states the strongest variables in predicting loss was the prior week's loss and the week of the water year, which was meant to be handled by the additional hurdle model fitting, but also why the more conservative LOO CV method was selected.

    - "...the 10-fold approach produced consistently more optimistic results (i.e., suggested better predictive performance). This probably results from the fact that there is an unaccounted-for effect of year in determining the risk of entrainment and removing an entire year of data ensures that the model is trained completely naively to this annual effect. For the sake of clarity, we have chosen to report only the results from the more conservative LOO cross-validation. "



```{r load_libraries, message=FALSE, warning=FALSE}
library(tidyverse)
library(here)
library(randomForest)
library(quantregForest)
library(plotly)
#load BOR shared resources
load(here("R/ITMData.rda")) #return 1) df.ss and 2) df.week from 1999 to 2021

# #shared model output from Nick Beer -- used to calibrate
load(here::here("data-raw", "FullFits.Original.R")) #seems to be model output for each of the three options: 1) Steelhead, 2) Winter-run Chinook, 3) Spring-run Chinook
```

```{r compare_train_datasets}

#seems to have all data foe every day of the week -- this likely was populated usng the kNNimputation() to fill out?
train_nb<-read.csv(here::here("data-raw/AllYears.Intake.csv")) %>%  
  mutate(date = lubridate::ymd(date))

# Identify dates in train.nb that are not in df.ss
missing_dates_train_nb <- anti_join(train_nb, df.ss, by = "date") %>% select(date) #8,234 obs in train.nb not in df.ss

# Identify dates in df.week that are not in df.ss
missing_dates_df_week <- anti_join(df.week, df.ss, by = "OMR") %>% select(OMR) #286 obs in df.week not in df.ss

#identify dates in train.nb that are not in df.week
missing_dates_train_week <- anti_join(train_nb, df.week, by = "date") %>% select(date) #7,948 obs in train.nb not in df.week
  
```

```{r import_new_loss_data}
# assign current water year
source(here("R/utils_fct_assign_current_water_year.R"))
current_year <- assign_current_water_year()

startOfWY <- function(date) {
  if (month(date) >= 10) {
    ymd(paste(year(date), "10-01", sep = "-"))
  } else {
    ymd(paste(year(date) - 1, "10-01", sep = "-"))
  }
}

calculateWYWeek <- function(date) {
  waterYearStart <- startOfWY(date)
  daysSinceStart <- as.integer(difftime(date, waterYearStart, units = "days"))
  weekNumber <- ceiling(daysSinceStart / 7)
  return(weekNumber)
}

weekStartDate <- function(weekNumber, currentDate) {
  waterYearStart <- startOfWY(currentDate)
  startDateOfWeek <- waterYearStart + days((weekNumber - 1) * 7)
  return(startDateOfWeek)
}

winter_run_url <- paste0("https://www.cbr.washington.edu/sacramento/data/php/rpt/juv_loss_detail.php?sc=1&outputFormat=csv&year=", current_year, "&species=1%3Af&dnaOnly=no&age=no")

df_fish_raw <- read_csv(winter_run_url) %>%
    janitor::clean_names()
  
df_fish_winter  <- df_fish_raw %>% 
  filter(lad_race == "Winter") %>% 
    mutate(
      sample_time = ymd_hms(sample_time),
      date = date(sample_time)
    ) %>%
    group_by(date) %>%
    summarise(total_daily_loss = sum(loss)) %>%
    ungroup() %>%
    mutate(week = sapply(date, calculateWYWeek)) %>%
    group_by(week) %>%
    summarise(total_weekly_loss = sum(total_daily_loss)) %>% 
    mutate(calendar_date = weekStartDate(week, Sys.Date()))  # Add calendar date column



```

```{r import_new_river_data}

#load function to import river data
source(here::here("data-raw/utils_fct_import_river_data.R"))

# Set variables of interest for river data import function
  # To get current WY years of data:
  today <- Sys.Date()
  # Determine the current and previous water years based on today's date
  if (format(today, "%m") >= "10") {
    currentWY <- as.numeric(format(today, "%Y")) + 1
  } else {
    currentWY <- as.numeric(format(today, "%Y"))
  }
  previousWY <- currentWY - 1
  # return years of interest
  years <- previousWY:currentWY

  # select sites of interest: Flow, OMR, Export, Temperature
  code_list <- c("FPT", "VNS", "MAL", "TRP", "HRO")
  # set years of interest
  years <- c(years)
  # selected metrics of interest
  metrics <- c("Flow", "WaterTemperature", "PumpingDischarge")

  # Run the function with the code list, years, and metrics
  df_river_raw <- fct_import_SacPAS_river_conditions_query(sites = code_list, years = years, metrics = metrics)
  # for some reason function returns OMR data multiple times when run in a list above. For now running seperately
  df_OMR_raw <- fct_import_SacPAS_river_conditions_query(sites = "Combined", years = years, metrics = "OMRDailyUSGS")

  df_river <- df_river_raw %>%
    bind_rows(df_OMR_raw) %>%
    mutate(WY = year(YMD) + (month(YMD) >= 10)) %>%
    filter(WY == current_year) %>%
    mutate(week = sapply(YMD, calculateWYWeek)) %>%
    pivot_wider(names_from = c(location, parameter), values_from = value) %>%
    select(YMD, WY, week, FPT_flow, VNS_flow, Combined_OMRDailyUSGS, TRP_pumping, HRO_pumping, MAL_wtemp) %>%
    mutate(combined_export = TRP_pumping + HRO_pumping) %>%
    group_by(week) %>%
    summarise(
      weekly_avg_fpt_flow = mean(FPT_flow, na.rm = TRUE),
      weekly_avg_vns_flow = mean(VNS_flow, na.rm = TRUE),
      weekly_avg_omr = mean(Combined_OMRDailyUSGS, na.rm = TRUE),
      weekly_avg_export = mean(combined_export, na.rm = TRUE),
      weekly_avg_mal_wtemp = mean(MAL_wtemp, na.rm = TRUE)
    )
  
  
```

## Shared Model Output

```{r track_a_cohort_output}
source(here("R/winter_run_chinook_plot_tillotson_predicted_loss.R"))

source(here("R/steelhead_plot_tillotson_predicted_loss.R"))
```


-------------------past code
```{r}

```


```{r}
#required to run model
# source(here("R/brt.functions.R")) #seems to use gbm to determine howpredicted values are returned - customized function?
load(here("R/ITMData.rda")) #seems to be the data used to train the model, return 1) df.ss and 2) df.week from 1999 to 2021



#removed all unused selections from hurdle.model function in spp_setup.R
hurdle.model <- function(data, response, 
                         quantile.predictors, 
                         predict.data = NA,
                         which.quant = .5,
                         n.trees = 500,
                         quantile.nodesize = 10,
                         log.quant = F,
                         ...) {
  resp.loc <- which(colnames(data) == response)
  
  # Use the entire dataset for quantile regression (if(pres.only == F))
  data.pres <- data.frame(data)
  
  # Train the quantile regression forest model (  if(quantile.form=="qrf"&binary.form!="none"))
  quantile.model <- quantregForest(x = data.pres[, quantile.predictors], y = data.pres[, resp.loc],
                                   ntree = n.trees,
                                   importance = TRUE, 
                                   proximity = TRUE,
                                   keep.forest = TRUE,
                                   keep.inbag = TRUE,
                                   rsq = TRUE,
                                   nodesize = quantile.nodesize)
  
  # Make predictions
  #if prediction data = NA, then use the data provided (training data)
  if (is.na(predict.data)) {
    prediction.a <- predict(quantile.model, newdata = data, what = which.quant)
    Pred_Mean <- predict(quantile.model, newdata = data, what = mean)
    #else, if prediction data is provided, use new data (test data)
  } else {
    prediction.a <- predict(quantile.model, newdata = predict.data, what = which.quant)
    Pred_Mean <- predict(quantile.model, newdata = predict.data, what = mean)
  }
  
  prediction <- data.frame(cbind(prediction.a, Pred_Mean))
  
  return(list(quantile.model, prediction))
}

```

```{r model_winterun}
#model run
names(df.ss)
#columns call for winter-run chinook  input to quantile.predictors
names(df.ss[c(2:4,17:21,27)])


#change dcc to open or closed instead of numerical values
df.ss <- df.ss%>%mutate(dcc= if_else(dcc==2,"open","closed"))
df.week <- df.week%>%mutate(dcc= if_else(dcc==2,"open","closed")) #attempting to try with df.week but getting error--why? start here


# run model
WR_Simple_Combined <- hurdle.model(data = df.ss,
                                      response = "winter",
                                      binary.predictors = NA,
                                      quantile.predictors = c(
                                        "wy_week", 
                                        "mal_temp",
                                        "precip", 
                                        "OMR", 
                                        "sac", 
                                        "sjr",
                                        "dcc",
                                        "daily_exports",
                                        "winter.pw"
                                        ),
                                      binary.form = "none",
                                      quantile.form = "qrf",
                                      which.quant = c(0.01,0.05,.1,.25,.5,.75,.9,.95,.99),
                                      n.trees=500,
                                      quantile.nodesize = 5,
                                      pres.only=F
)


#combine fish and river for winter
df_combined_winter <- df_fish_winter %>%
    left_join(df_river, by = "week")

# Initialize a list to store predictions
predictions_list <- list()

# Loop through each row in df_combined_winter to make predictions
for (i in 1:nrow(df_combined_winter)) {
  NewData <- data.frame(
    wy_week = df_combined_winter$week[i] + 1,
    mal_temp = df_combined_winter$weekly_avg_mal_wtemp[i],
    precip = 1,
    OMR = df_combined_winter$weekly_avg_omr[i],
    sac = df_combined_winter$weekly_avg_fpt_flow[i],
    sjr = df_combined_winter$weekly_avg_vns_flow[i],
    dcc = "closed",
    daily_exports = df_combined_winter$weekly_avg_export[i],
    species.pw = df_combined_winter$total_weekly_loss[i]
  )

  # Dynamically rename the species.pw column
  names(NewData)[names(NewData) == "species.pw"] <- "winter.pw"
  
  # Access the quantile regression forest model from the list
  quantile_model <- WR_Simple_Combined[[1]] #changed to 1 versus 2 -- why did call change in listed value for WR_Simple_Combined?
  
  # Make predictions using the quantile regression forest model
  pred <- predict(quantile_model, newdata = NewData, what = c(0.05, 0.5, 0.95))
  predictions <- data.frame(
    lowerCI = pred[, 1], median = pred[, 2], upperCI = pred[, 3],
    week = df_combined_winter$week[i],
    OMR = df_combined_winter$weekly_avg_omr[i],
    Export = df_combined_winter$weekly_avg_export[i],
    ObservedLoss = df_combined_winter$total_weekly_loss[i],
    weekly_avg_mal_wtemp = df_combined_winter$weekly_avg_mal_wtemp[i],
    weekly_avg_fpt_flow = df_combined_winter$weekly_avg_fpt_flow[i],
    weekly_avg_vns_flow = df_combined_winter$weekly_avg_vns_flow[i],
    calendar_date = df_combined_winter$calendar_date[i]  # Include calendar date in predictions
  )
  
  # Store the predictions in the list
  predictions_list[[i]] <- predictions
}

# Combine all predictions into a single data frame
final_predictions <- bind_rows(predictions_list)

# Print the final predictions
print(final_predictions)

```

```{r model_steelhead}
#column calls for steelhead input to quantile.predictors
names(df.ss[c(2:4,17:21,29)])


Stlhd_Simple_Combined <- hurdle.model(data = df.ss,
                                      response = "stlhd_loss",
                                      binary.predictors = NA,
                                      quantile.predictors = c(
                                        "wy_week", 
                                        "mal_temp",
                                        "precip", 
                                        "OMR", 
                                        "sac", 
                                        "sjr",
                                        "dcc",
                                        "daily_exports",
                                        "stlhd_loss.pw"
                                        ),
                                      binary.form = "none",
                                      quantile.form = "qrf",
                                      which.quant = c(0.01,0.05,.1,.25,.5,.75,.9,.95,.99),
                                      n.trees=500,
                                      quantile.nodesize = 5,
                                      pres.only=F
)


```
